System Design
System design is the process of designing the elements of a system such as architecture, modules and components, the different interfaces of those components and the data that goes through that system.

Types of System Design
HLD  -  High-Level Design
Describes the main components that would be developed for the resulting product.
The system architecture details, database design, services and processes, the relationship between various modules and features.

LLD  -  Low-Level Design
Describes the design of each element mentioned in the High-Level Design of the system.
Classes, interfaces, relationships between different classes, and actual logic of the various components.




Monolithic Architecture
Architecture - Internal design details for building the applications.
Web Application with Front-End, Back-End and Data Storage written and deployed together is called monolithic architecture.

If all the components and functionalities of a project are entangled and combined in a single codebase, then that is a monolithic application.
Monolithic architecture has less complexity ---> Easier to understand --> Higher productivity
Monolithic system is also known as centralized system.

Advantages
Best architecture if you're a beginner and just starting.
Since all modules are present in the single system, they require fewer network calls as compared to other architectures.
Comparatively easier to secure monolithic system.
Integration testing is easier and lesser confusion.

Disadvantages
In monolithic architecture, every module is combined in a single system, so if there is an error or bug in a single module, it can destroy the complete system.
Whenever a single module is updated, the whole system needs to be updated to reflect the changes to user. All modules are present in a single system and are connected to one another, so the whole system needs to be updated.
If there is any change in a single module's programming language or framework, it can affect the entire system. The entire system needs to be changed because every module is interlinked and tightly coupled.




Distributed System
A distributed system is a collection of multiple individual systems connected through a network that share resources, communicate and coordinate to achieve common goals.

Advantages
Scalable : Easy to scale horizontally(We can add more machines to improve scalability)
No Single Point of Failure
Low Latency

Disadvantages
Complex as there are multiple servers involved
Additional management required to handle resources(Load balancing etc)
Difficult to secure
Messages may be lost in between nodes




Latency
Latency = Network delay + Computational delay
Monolithic doesn't involve network delay and hence is faster than distributed systems

Reducing Latency
1. Caching : Add a caching layer in b/w user and server
Caching is the process of storing information for a set period of time on a computer

2. Continuous Delivery Network(CDN) : Used to store static data that doesn't change often
CDNs are geographically distributed networks of proxy servers and their objective is to serve content to users more quickly.

3. Upgrading : Upgrading of hardware




Throughput
The volume of work or information flowing through a system.

Throughput is the amount of data transmitted per unit of time.
It is the process flow rate.
Throughput is measured in bits per second, i.e, bps.

Throughput will be more in distributed systems.(No limit in resources, load balancer)

Causes of Low Throughput
Latency
Protocol Overhead(Handshake needed b/w two servers)
Congestion(All requests coming at the same time)

Improving Throughput
CDN
Caching
Distributed system
Load balancer
Improve resources of system




Availability
Fault tolerance is directly proportional to availability.
Distributed systems have higher availabilty than monolithic architecture.

How to increase availability?
1. Replication
Replication includes dependency, but involves the copying of data from one node to another or the synchronization of state between nodes.

2. Distributed systems

3. Redundancy
Redundancy is the duplication of nodes, in case of some of them are failing.




Consistency
When more than one client requests the system, for all such requests, it will be called consistent when each client gets the same data. 
The data should always be consistent, regardless of who is accessing it.

When more than one client requests the system, for all such requests, when different clients get different responses due to some recent update that has not been committed to all systems yet, this reading operation will be called dirty read.

Factors improving consistency
1. Improving network bandwidth
2. Stop the read - Stop the read operation, until all servers are updated and synchronized
3. Replication based on Distance aware strategies - Place servers that require synchronization close to each other

Types of consistency
1. Strong consistency
When the system doesn't allow read operation until all the nodes are with replicated data are updated.

2. Eventual Consistency
User Read Requests are not halted till all the replicas are updated rather the update process is eventual. Some users might recieve old data but eventually all the data is updated to the latest data.

3. Weak Consistency
No need for synchronization, may or may not happen.




CAP Theorem
CAP --> Consistency, Availability and Partition Tolerance

For a distributed system, the CAP Theorem states that it is possible to attain only two properties and the third would be always compromised.
The system requirements should define which two properties should be chosen over the rest.

The system designer can select Consistency and Partition Tolerance but the Availability would be compromised then.
  Should avoid dirty read scenario, hence availability will be compromised for some time until synchronized.

The system designer can select Partition Tolerance and Availability but the Consistency would be compromised then.
  In social media, if a user posts now and immediately if its not reflected instantly in your feed it is not as issue.

The system designer can select Availability and Consistency but the Partition Tolerance would be compromised then.
  System will be centralized, one node, monolithic architecture.

Partition Tolerance cannot be compromised and hence when designing systems it comes down to either compromising Consistency or Availability.
Eg : What can be compromised in below systems?
a) Blogs website -> Consistency
b) Multiplayer online games -> Consistency
c) Stock trading platforms -> Availability
d) Video Streaming sites -> Consistency
e) Ticket Booking System -> Availability
f) Video chat applications -> Consistency
g) Bank -> Availability




Lamport Logical Clock
Lamport's Logical Clock is a mechanism for determining the order of events in a distributed system, where events can occur concurrently and there is no global clock.

How It Works:
Each process maintains a counter (logical clock) that starts at zero.
Before an event occurs (like sending a message), the process increments its counter.
When a process sends a message, it includes its current timestamp.
Upon receiving a message, the receiving process updates its counter to be greater than its current value and the timestamp in the message.




Scalability
Scalability refers to the ability of a system, network, or process to handle a growing amount of work, or its potential to accommodate growth.

Vertical Scalability
Pros:
Involves adding more resources (CPU, RAM, storage) to an existing machine.
Easy implementation.
Less power.(Only one machine is scaled and hence lesses power)
Management is easy.

Cons:
Single point of failure.
There is a limit for upgrading.

Horizontal Scaling
Involves adding more machines or nodes to a system.
Typically more complex but can provide greater resilience and capacity.

Pros:
No Single point of failure.

Cons:
Management is difficult.
Power consumption is higher.
Securing applications is also tougher.




Redundancy
Redundancy is simply the duplication of nodes or components so that when a node or component fails, the duplicate node is available to service components.

Active Redundancy is considered when each unit is operating/active and responding to the action. Multiple nodes are connected to a load balancer and each unit receives an equal load

Passive Redundancy is considered when one node is active or operational and the other is not operating. During the breakdown of the active node, the passive node maintains availability by becoming the active node.




Replication
Replication = Redundancy + Synchronization

Active Replication
If a read-write operation is happening in a DB which is hosted on multiple nodes, read-write will happen in all the nodes and become synchronized.

Passive Replication
Every read-write -> master
If master goes down one slave becomes master.

Master-slave replication can be either synchronous or asynchronous. The difference is simply the timing of propagation of changes.
If the changes are made to the master and slave at the same time, it is synchronous.
If changes are queued up and written later, it is asynchronous.




Load Balancers
Load balancing is the process of efficient distribution of network traffic across all nodes in a distributed system.

Roles of load balancer
1. The load distribution is equal over every node.
2. Health check(if the node is not operational, the request is passed to another node that is up and running)
3. Load balancers ensure high scalability, high throughput and high availability.

When to use it and when not to use it?
In monolithic or in case of vertically scaled system, we don't need it. But in microservice architecture, we need it.

Challenges of load balancing
Single Point of Failure:
During a load balancer malfunctioning, the communication between clients and servers would be broken.
To solve this issue, we can use redundancy. The system can have an active load balancer and one passive load balancer.

Advantages of using load balancer
1. Optimization : Load balancers help in resource utlilization and lower response time, thereby optimizing the system in a high traffic environment
2. Better user experience : Load balancers help in reducing the latency and increasing availability making the user's request go smoothly and error-free.
3. Prevents Downtime : Load balancers maintain a record of servers that are non-operational and distribute the traffic accordingly, therefore ensuring security and preventing downtime, which also increases profit and productivity.
4. Flexibility : Load balancers have the flexibility to re-route the traffic in case of breakdown and work on server maintainance to ensure efficiency. This helps in avoiding traffic bottlenecks.
5. Scalability : When the traffic of a web application increases suddenly, load balancers can use physical and virtual servers to deliver the responses without any disruption.
6. Redundancy : Load balancing also provides in-build redundancy by re-routing the traffic in case of any failure.

Load Balancing Algorithms
1. Round Robin (Static) : Rotation fashion
2. Weighted Round Robin (Static) : It is similar to Round Robin when the servers are of different capacities.(some node can have better resources, others might not have)  
3. IP Hash Algorithm (Static) : The servers have almost equal capacity, and the hash function(input is source IP) is used for random or unbiased distribution of requests to the nodes.
4. Source IP Hash (Static) : Source IP Hash combines the server and client's source and destination IP addresses  to produce a hash key. The key can be used to determine the request distribution.
5. Least Connection Algorithm (Dynamic): Client requests are distributed to the application server with the least number of active connections at the time the client request is recieved.
6. Least Response Time (Dynamic) : The request is distributed based on the server which has the least response time.




Caching
Caching is a technique used to temporarily store frequently accessed data in a high-speed storage layer (such as memory) to improve retrieval times and reduce the load on primary data sources.

Types of Caching
1. In-memory/Local cache - Cache content locally in the server. Eg: MemCache
2. Distributed/Extrenal Cache Eg : Redis

When to use?
1. When the app is read intensive
2. When the app has lot of static content

Content Delivery Network(CDN)
CDNs are geographically distributed networks of proxy servers and their objective is to serve content to users more quickly. Mostly used to store static contents.

Cache Eviction Strategies
Cache Miss : API tries to fetch data from Cache, but not available.

1. Least Recently Used(LRU) : Remove the leastly used data stored in cache.
2. Most Recently Used(MRU) : Remove the most recently used data stored in cache.
3. Least Frequently Used(LFU) : Remove the leastly accessed data stored in cache.
4. First In First Out(FIFO) : If there is a limit set in cache and data has reached the limit, remove the first cached data when a new data comes.
5. Last In First Out(LIFO) : If there is a limit set in cache and data has reached the limit, remove the last cached data when a new data comes.
6. Random Replacement(RR) : Randomly evict data from cache.




File Based Storage System
A file based storage system is a database management system where data is stored in the form of files.

Challenges of File Based Storage System
1. Data Redundancy : Update anomaly, delete anomaly leading to data inconsistency
2. Poor Security : Same data are present in the system which causes a possibility of data breaches by unauthorized users.
3. Slow Speed : Speed is very slow and data retrieval is not very efficient.

To overcome these challenges we use relational database management systems.(RDBMS)




Relational Database Management Systems(RDBMS)
Definiton of RDBMS
Software - That performs data operations on relational database.
Operations - Store, manage, query and retrieve data.
Tables - Data is represented in the form of tables.
Foreign Keys - The relationship between the two tables is represented by foreign keys.

Advantages
No data redundancy and inconsistency.
Data Concurrency : A locking system is provided by RDBMS to prevent abnormalities from occuring.
Data Searching : Built-in searching capabilities(No need of a seperate programme as in FS(File Based Storage System))
Data Integrity : Eg: To maintain data integrity, numeric columns won't have alphabet data.(There is no process in the File Based Storage System to check these constraints automatically)

Problems
Rigid Schema : No flexibility and we should follow the structure that is defined.
High Cost
Scalability Issues : Horizontal scaling/ sharding is very difficult.




NoSQL
NoSQL stands for "non-SQL" database or we can say that it is a non-relational database.

NoSQL is the umbrella term comprising of four different types of databases.
1. Key value DB
Generally used for caching. 
Eg: Redis

2. Document DB
Brings best of both RDBMS and NoSQL.
It combines the relationship concept from RDBMS and dynamic schema and horizontal scaling from NoSQL databases.
Eg: MongoDB

3. Columnar DB
The columns are stored together instead of rows.
Because of that, the aggregations in such databases is rapid. It is widely used for Data Analysis.
Used when we query on a subset of your data's columns.
Because columnar DB just needs to read these specific columns, it conducts such queries quickly(while row-based DB would have to read the entire data)
Eg : Cassandra

4. Graph DB
Graph database represents and stores entities and relationships in the form of graph data structure. It is mainly used for social networks.
Eg: Neo4j

DB Selection based on app requirement :
Google Maps : Graph DB
LinkedIn : Graph DB
Shopping Website Cart Page : Key Value DB
Score Card : Key Value DB
Machine Learning and Data Analysis : Columnar DB
Payments : Should be consistent, hence should not use NoSQL, always use RDBMS.

Polyglot Persistence
If an application's needs cannot be satisfied by a single type of DB, we use multiple DBs. This is called polyglot persistence.

Eg:                      E-Commerce Platform
                                  |
-------------------------------------------------------------------------
|                          |                         |                  |
Shopping Cart            Completed               Inventory         Customer Social Graph
    &                      Order                     &                  |
Session Data               |                     Item Price           Graph Store 
    |                    Document Store              |
Key Value Store                                     RDMS
                                                  (LegacyDB)

Normalization and Denormalization
Putting data into multiple tables to avoid redundancy is called normalization.
Eg : For Student and Department Relation, we create a table called department for department details and add the dept_id in student table to avoid redundant department data in Student table.

Denormalization combines the data and organizes it in a single table.
It is the process of adding redundant data to the normalized relational database to optimize its performance.

Benefits of Denormalization
1. Faster read and write operations.
2. Management convenience.(Can avoid joins)
3. High data availability.
4. Reduces the number of network calls to fetch data from multiple places.

Challenges of Denormalization
1. Redundant data - Wastage of memory.
2. It increases the complexity.
3. Data inconsistency.
4. It will slow read write operations since we will need to write multiple places due to redundancy.

Indexing
Indexing creates a lookup table with the column and the pointer to the memory location of the row, containing this column.
When we create an index for a column in a table, it stores in a seperate memory location the sorted column and pointer to the table row.

B-Trees data structure is used to store the indexing as it is a multilevel format of tree-based indexing, which has balanced binary search tress.
Before indexing searching queries had a time complexity of O(n), after indexing, it becomes O(log n).

Indexing shouldn't be added for all tables, it should be added for tables which are read-intensive.
Also, if a table is write-intensive in nature, we shouldn't use indexing.(For every insertion, at the seperate memory location where indexing table is present we need to do an insertion and perform sorting)




Synchronous communication(Blocking call)
We block our time and wait for some process to be completed to perform further tasks.
A client sends a request to the server and will wait until the response is received to perform any other request.
Used to achive consistency and during transactions.

Industrial Use Cases :
1. Stock Market
2. Bank Payments
3. Ticket Booking
4. Real Time Decision Making

Asynchronous Communication(Non Blocking Call)
We don't need to block our time and can perform other operations.
No need to wait for the response from the server.

Scenarios :
Adding an item to Amazon Cart - Synchronous Communication - Application waits for this response before allowing the user to add the project into the cart.
Proceeding with payment - Synchronous Communication - Application waits till the confirmation of payment has been successfully received from the bank. 
Sending order placed notification - Asynchronous Communication - It can take some time for the notification to arrive, but in the meantime, a user can do anything on the application without waiting for the notification.

Where is it necessary? (Asynchronous Communication)
1. Computation takes a lot of time
2. Scalability of application
3. Avoid cascading failure

Message Based Communication
Client sends request in the form of a message.
Receives response in the form of a message.
It is async, so client is not required to halt or wait for the process.

It has a producer(produces message), consumer(consumes message) and agent(medium/channel through which message is passed).
Agent is generally a queue which follows FIFO principle.
Eg : Kafka, RabbitMQ

P2P Model(Peer to Peer)
Two parties are involved.
Eg: When sending a mail to another person, it takes some time to receive the mail as the communication is async.

Publish Subscribe Model
Multiple parties are involved.
1 producer but multiple consumers.
Eg: Subsciption to newsletters




Web Server
Tools or programs that help keep the web application always up and running.
It can refer to hardware or software, or both of them working together.

On the hardware side, a web server is a computer that stores web server software and a website's component files(for example, HTML documents, images, CSS stylesheets and JavaScript files). 
A web server connects to the Internet and supports physical data interchange with other devices connected on web.

On the software side, a web server includes several parts that control how web users access hosted files.
At a minimum, this is an HTTP server.
An HTTP server is a software that understands URLs(web addresses) and HTTP(the protocol your browser uses to view webpages).
An HTTP server can be accessed through the domain names of the website it stores, and it delivers the content of these hosted websites to the end user's device.

At the most basic level, whenever a browser needs a file that is hosted on a web server, the browser requests the file via HTTP.
When the request reaches the correct (hardware) web server, the (software) HTTP server accepts the request, finds the requested document, and sends it back to the browser, also through HTTP.(If the server doesn't find the requested document, it returns a 404 response instead)




Communication Protocols
The rules, syntax, semantics and timing of communication and possible error recovery methods.
These protocols can be implemented using hardware, software or a combination of both.




Models
1. Push
The client opens the connection with the server and keeps it always active.
The server pushes new events to the client.
This method reduces the server load.

2. Pull/Polling
Client request, server responds.
What if client sends 100 requests but get response to just 5 request, that's where Long Polling comes into picture.   

3. Long Polling
Client requests, server keeps it open until it gives response.

Disadvantages:
Ordering issue
Server will be always busy.(To avoid this we use push model)

4. Socket
Used when we need continuous frequent connections.
A socket is an endpoint of a two-way connection link between two servers/nodes over a network.(Two way connection channel)
Eg: Whatsapp

5. Server sent events
Client subscribes to the server "stream", and the server will send a message ("stream of events") to the client until the server or client closes the stream.
One-way connection.
Long-Lived connection.
Eg: Cricbuzz

Push API allows the server to send a notification to a client even when your site is not open, because it relies on service workers.
SSE(or Websockets) work as longer as the user is using your site.




Web Application
Application that runs on the internet.

Client: 
Who requests 
System that initiates the communication over a network. 
Eg: MobileApps, Web based consoles, laptops etc
A web server can also act as a client when requesting information.

Server: 
Who provides response 
System that receives the request over the network.

What is required for communication to happen between client and server?
1. Language independent
2. Fast
3. Enable communication over the network
4. Light weighted

REST(REpresentational State Transfer)
A technique/style/standard to provide the above 4 requirements.
To use the standard, we expect the server that it should expose REST API.

The request to the REST API can be of 4 types:
POST -> CREATE
GET -> READ
PUT -> UPDATE
DELETE -> DELETE

REST API = HTTP VERB + URI

SOA(Service Oriented Architecture)
Service Oriented Architecture is a style of architecture that promotes loose coupling and granular applications to make the components of the software reusable.

Advantages:
1. Selective scaling - If app has 1000s of APIs but only 1 API is used frequently, we cannot scale if it is a monolith as it would be wastage of resource.
2. Different Tech Stacks - Communication is using jsons even though apps are build using different tech stacks.
3. Loose Coupling - App is modularized and deployed independently.
4. Agile - Development can be done by different teams.

Disadvantages:
1. Higher latency - Process to process network calls increases latency.
2. Complex to secure - Different components on different machines, will be complex to provide security.
3. Cascading failures - If a service fails, there are chances for other dependent services to fail.
4. Complex understanding - Complex since different technologies are involved.

Microservices Architecture
Microservices Architecture is an evolved version of SOA that promotes software components to be loosely coupled.
It is the most granulated type of architecture design and every service is completely independent of the other.

In SOA if there are two dependent services, we make 5 copies of each and deploy on 5 machines.
But in microservices, we deploy all services serperately in 10 machines to avoid unwanted dependency.

SOA                                                                       Microservices
Services can share storage                                                Each microservice has seperate and independent data storage
Less scalable architecture                                                Highly scalable architecture
Deployment is time-consuming                                              Deployment is easy and less time-consuming
Focused on maximizing application service reusability                     More focused on decoupling

Tier Architecture
A web application can be designed according to the n-tier architecture where tiers are different layers of architecture.
A tier is a logical seperation between different components of the application.
Tier architecture helps make modifications and updation of different components easy.
It helps in assigning dedicated tasks and roles to each component.

Eg :
1-tier ==> Frontend, backend and DB in same machine
2-tier ==> Frontend in a machine, backend and DB in same machine
3-tier ==> Frontend, backend and DB all are in different machines.




Authentication vs Authorization
Authentication ==> Who are you? (Login to a DB with IP and credentials)
Authorization ==> What can you do? (Permission to access DB might be read-only, write-only etc)

Basic Authentication
Client will register to an application with a username and password.
Now with every request, client will share this username and password to fetch the response.
This is the most basic version of authentication.
The main disadvantage is that we are passing credentials in plain text for every request.

Token based Authentication
Client will register to an application with a username and password.
Now when client logins to app with credentials, app will provide a token.
Now with every request, the token is sent to the app.
Token will expire after a specified time.

OAuth Authentication
There are so many applications available and it is difficult to remember all their credentials.
Also when a new app is launched, and for registering a long form is placed, user won't be in the mood to fill all the details.
User just wants to sign up quickly with the click of a single button.
That's where we use OAuth where user uses a third party applicatin to authenticate and create a token for accessing the new application without any credentials.




Proxies
A proxy server is a hardware or a piece of software that is placed between a client and an application to provide intermediary services in the communication.
Proxy server provides a gateway between the user and the Internet.

Forward Proxy
A forward proxy server is placed in between a client and server.
The client request hits the proxy server first and then reaches the application.
Forward proxy hides the identity of the client from the server.
Server doesn't know the client.

Reverse Proxy
Forward proxy hides the identity of the client, but reverse proxy hides the identity of the server.

When to use reverse proxy?
1. When I don't want to expose the server.
2. To make user feel there is only one server.
3. Provide load balancing for the server.
4. Allow administrators to swap the backend servers without disturbing the traffic.
5. Filter out some requests.




URL Shortener System Design(TinyURL, Bitly)
What to discuss?
1. Features(Length of URL, domain)
2. Estimates(Memory, CPU Requirements)
3. Design goal(CAP)
4. HLD
5. Scaling

Functional and Non-Functional Requirements
Functional:                                                                     Non-Functional:
1. Shorten                                                                      1. High availability
2. Redirection                                                                  2. Low latency
3. Expiry                                                                       3. URLs should not be predictable

Estimating the system capacity
RAM, Storage etc

Read heavy or Write heavy
URL Shortener will be read heavy in nature

Requirement from interviewer
100:1 Ratio of read and write calls

Every month -> 1 million new URLs
Every month -> 100 million redirections

Query Per Second will be :
QPS = 100,000,000/30*24*3600 = 50 qps

Expiry can be set as 10 years
==> Total URLs in 10 years = 1,000,000 * 12 * 10

Consider each URL size as 500 bytes
==> Total size in bytes = 1,000,000 * 12 * 10 * 500 --> approximately 60GB(Main memory)

No. of queries per day = 50 * 24 * 3600 --> approximately 5 million qpd

We'll use caching and use 1 day time to live(TTL) for the data.

Assume lets cache 25% of 5 million queries in cache ==> approximately 1.25 million queries
Hence, 1.25 million * 500 bytes = 1GB(Secondary memory)

Design goal
1. Read intensive
2. Highly available
3. Low latency
4. Security(Not everyone can shorten URL)

HLD of URL Shortner
APIs
2 main APIs are needed. One to shorten and another to delete URL

String shortenUrl(String originalUrl, long userId, String apiKey, Date expiry);
Boolean deleteUrl(String apiKey, String originalUrl, String shortUrl);
//Signup, Login, Logout
//Redirection

Database
Let's use MongoDB since we don't need relational databases. DB should be easily scalable and available.

Create two collections,
1. URL
    |
    -----------------------------------------------------------------------------
    |                  |                  |                  |                  |
original_url       shorten_url        user_id            expiry_date         created_at

2. User
    |
    -----------------------------------------------------------------------------
    |                   |                  |                  |                 |
  user_id              name              email             api_key           created_at

Algorithm
Main requirement is to convert long URL to short URL.
Also we need to avoid collisions.

Lets try to use MD5 algorithm to shorten the URL.
We provide the long URL and MD5 will process and provide a short unique hash value of size 128 bits.
The MD5 (Message Digest Algorithm 5) hash produces a fixed-size output of 128 bits, which is typically represented as a 32-character hexadecimal string. Each hexadecimal character represents 4 bits, so 32 characters Ã— 4 bits = 128 bits.
But we don't need 32 characters long value, we just need the first 6 digits of the hashed value.
When fetching just the first 6 characters, there are chances of 2 URLs to have the same 6 characters.(Collision)
Hence cannot use MD5.

Characters that can be available in shortened URL : a-z, A-Z, 0-9 ==> 62 characters
==> 62^n unique combinations are possible(n is the number of characters permitted in shortened URL)

Therefore, no of unique URLs that can be generated should be greater than no of URLs that can be stored
62^n > 1,000,000 * 12 * 10
==> n = log62(1,000,000 * 12 * 10)
==> n = 4.5
Take the ceiling value
==> n = 5

To satisfy the interviewers requirement to handle 1 million new URLs, we can use n=5. This is sufficient.

Let's take n = 7
==> 62^7 = 3.5 trillion

Since we can't use MD5, lets use Base 62 algorithm
Number ---> Base 62 ---> AZaz12..  

Base 62 expects a number as input and then generates a combination of digits A-Z, a-z, 0-9
When we fetch a URL a unique number is generated which will be passed to Base62 and we generate a unique value of length 7.
We need to confirm that the generated random unique number shouldn't be present in DB. If present, generate another random number.

In MongoDB, there are no constaints, hence it is difficult to check unique number condition in DB.

There comes Redis, where we can store a counter, which can be incremented when each new URL comes, but it results in single point of failure, i.e, if redis fails, whole system will fail. Even if we replicate Redis also we need to be consistent, which is difficult. So this approach use of Redis is also not possible. 

Let's use two Redis(suppose there are 2 URL shortener services), each storing a counter within a range specified. This way we can acheive consistency and avoid single point of failure. But the problem is if we have to add a new Redis, we need to manage the ranges specified in the two Redises and ensure that there is no overlap of ranges.

There comes Apache ZooKeeper, where we can specify the range for each service to use.

Design
                                 Apache    <--->  ZooKeeper                 
                                ZooKeeper          Replica 
                                 / \   |
                                  |    |
                                  |   \ /
                          |---  Service 1    ---->   Redis
                          |                  <----   (LRU Cache Eviction)
User -->  Load Balancer --|---  Service 2
                          |                   ----->      Mongo  <------>  Mongo
                          |---  Service 3     <-----       DB1   <------>   DB2
                                                          (Consistent hash partitioning and read write replicas)




Dropbox/Google Drive System Design
Functional and Non-Functional Requirement
Functional                                    Non-Functional
1. Upload                                     1. Scalable
2. Download                                   2. Highly available
3. Share                                      3. Low Latency
                                              4. Optimize data transfer by minimizing bandwidth
                                              5. ACID(Atomicity, Consistency, Isolation, Durability)

Estimating the system capacity
1 billion users
Average user stores 20GB
==> Total space = 20 Billion GB(20 Exabyte)

HLD
              |------------> Storage layer(Distributed storage) <----------------------|
              |                                                                        |
              |                                                                        |
Client -------|------------> Metadata storage layer                                    |
                                  / \                                                  |    
                                   |                                                   |
                             Synchronizer ---------------------------------------------|

When we upload a file, we are uploading in a folder which will reflect in the storage layer.
Storage layer will be a distributed system(Amazon S3, HDFS(Hadoop Distributed File System), Google File System)
The files uploaded will have some metadata, which will be stored in Metadata storage layer.
Synchronizer will synchronise changes made in a file in both metadata storage layer and storage layer across all devices and users.

Client:
1. Upload the file
2. Download the file
3. Detect any changes
4. Break the file into smaller changes thereby optimizing the file transfer.(Now only file changes has to be transferred instead of whole file)

Client can be divided into 4 parts :
1. Chunker - Calculate hash of each chunk and upload to x
2. Watcher - Notifies Chunker and Indexer about changes
3. InternalDB - Stores hashes, URL and metadata
4. Indexer - Takes hash and URL of chunks from chunker and save to InternalDB

Storage Layer:
1. Highly distributed(HDFS, GFS)
2. Add caching

Metadata Storage Layer
1. Permissions
2. Devices List
3. Info of chunks(chunk_id, chunk_order, etc)
4. Workspace folders
5. Versions
6. Hashes
