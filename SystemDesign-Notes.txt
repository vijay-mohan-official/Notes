System Design
System design is the process of designing the elements of a system such as architecture, modules and components, the different interfaces of those components and the data that goes through that system.

Types of System Design
HLD  -  High-Level Design
Describes the main components that would be developed for the resulting product.
The system architecture details, database design, services and processes, the relationship between various modules and features.

LLD  -  Low-Level Design
Describes the design of each element mentioned in the High-Level Design of the system.
Classes, interfaces, relationships between different classes, and actual logic of the various components.




Monolithic Architecture
Architecture - Internal design details for building the applications.
Web Application with Front-End, Back-End and Data Storage written and deployed together is called monolithic architecture.

If all the components and functionalities of a project are entangled and combined in a single codebase, then that is a monolithic application.
Monolithic architecture has less complexity ---> Easier to understand --> Higher productivity
Monolithic system is also known as centralized system.

Advantages
Best architecture if you're a beginner and just starting.
Since all modules are present in the single system, they require fewer network calls as compared to other architectures.
Comparatively easier to secure monolithic system.
Integration testing is easier and lesser confusion.

Disadvantages
In monolithic architecture, every module is combined in a single system, so if there is an error or bug in a single module, it can destroy the complete system.
Whenever a single module is updated, the whole system needs to be updated to reflect the changes to user. All modules are present in a single system and are connected to one another, so the whole system needs to be updated.
If there is any change in a single module's programming language or framework, it can affect the entire system. The entire system needs to be changed because every module is interlinked and tightly coupled.




Distributed System
A distributed system is a collection of multiple individual systems connected through a network that share resources, communicate and coordinate to achieve common goals.

Advantages
Scalable : Easy to scale horizontally(We can add more machines to improve scalability)
No Single Point of Failure
Low Latency

Disadvantages
Complex as there are multiple servers involved
Additional management required to handle resources(Load balancing etc)
Difficult to secure
Messages may be lost in between nodes




Latency
Latency = Network delay + Computational delay
Monolithic doesn't involve network delay and hence is faster than distributed systems

Reducing Latency
1. Caching : Add a caching layer in b/w user and server
Caching is the process of storing information for a set period of time on a computer

2. Continuous Delivery Network(CDN) : Used to store static data that doesn't change often
CDNs are geographically distributed networks of proxy servers and their objective is to serve content to users more quickly.

3. Upgrading : Upgrading of hardware




Throughput
The volume of work or information flowing through a system.

Throughput is the amount of data transmitted per unit of time.
It is the process flow rate.
Throughput is measured in bits per second, i.e, bps.

Throughput will be more in distributed systems.(No limit in resources, load balancer)

Causes of Low Throughput
Latency
Protocol Overhead(Handshake needed b/w two servers)
Congestion(All requests coming at the same time)

Improving Throughput
CDN
Caching
Distributed system
Load balancer
Improve resources of system




Availability
Fault tolerance is directly proportional to availability.
Distributed systems have higher availabilty than monolithic architecture.

How to increase availability?
1. Replication
Replication includes dependency, but involves the copying of data from one node to another or the synchronization of state between nodes.

2. Distributed systems

3. Redundancy
Redundancy is the duplication of nodes, in case of some of them are failing.




Consistency
When more than one client requests the system, for all such requests, it will be called consistent when each client gets the same data. 
The data should always be consistent, regardless of who is accessing it.

When more than one client requests the system, for all such requests, when different clients get different responses due to some recent update that has not been committed to all systems yet, this reading operation will be called dirty read.

Factors improving consistency
1. Improving network bandwidth
2. Stop the read - Stop the read operation, until all servers are updated and synchronized
3. Replication based on Distance aware strategies - Place servers that require synchronization close to each other

Types of consistency
1. Strong consistency
When the system doesn't allow read operation until all the nodes are with replicated data are updated.

2. Eventual Consistency
User Read Requests are not halted till all the replicas are updated rather the update process is eventual. Some users might recieve old data but eventually all the data is updated to the latest data.

3. Weak Consistency
No need for synchronization, may or may not happen.




CAP Theorem
CAP --> Consistency, Availability and Partition Tolerance

For a distributed system, the CAP Theorem states that it is possible to attain only two properties and the third would be always compromised.
The system requirements should define which two properties should be chosen over the rest.

The system designer can select Consistency and Partition Tolerance but the Availability would be compromised then.
  Should avoid dirty read scenario, hence availability will be compromised for some time until synchronized.

The system designer can select Partition Tolerance and Availability but the Consistency would be compromised then.
  In social media, if a user posts now and immediately if its not reflected instantly in your feed it is not as issue.

The system designer can select Availability and Consistency but the Partition Tolerance would be compromised then.
  System will be centralized, one node, monolithic architecture.

Partition Tolerance cannot be compromised and hence when designing systems it comes down to either compromising Consistency or Availability.
Eg : What can be compromised in below systems?
a) Blogs website -> Consistency
b) Multiplayer online games -> Consistency
c) Stock trading platforms -> Availability
d) Video Streaming sites -> Consistency
e) Ticket Booking System -> Availability
f) Video chat applications -> Consistency
g) Bank -> Availability




Lamport Logical Clock
Lamport's Logical Clock is a mechanism for determining the order of events in a distributed system, where events can occur concurrently and there is no global clock.

How It Works:
Each process maintains a counter (logical clock) that starts at zero.
Before an event occurs (like sending a message), the process increments its counter.
When a process sends a message, it includes its current timestamp.
Upon receiving a message, the receiving process updates its counter to be greater than its current value and the timestamp in the message.




Scalability
Scalability refers to the ability of a system, network, or process to handle a growing amount of work, or its potential to accommodate growth.

Vertical Scalability
Pros:
Involves adding more resources (CPU, RAM, storage) to an existing machine.
Easy implementation.
Less power.(Only one machine is scaled and hence lesses power)
Management is easy.

Cons:
Single point of failure.
There is a limit for upgrading.

Horizontal Scaling
Involves adding more machines or nodes to a system.
Typically more complex but can provide greater resilience and capacity.

Pros:
No Single point of failure.

Cons:
Management is difficult.
Power consumption is higher.
Securing applications is also tougher.




Redundancy
Redundancy is simply the duplication of nodes or components so that when a node or component fails, the duplicate node is available to service components.

Active Redundancy is considered when each unit is operating/active and responding to the action. Multiple nodes are connected to a load balancer and each unit receives an equal load

Passive Redundancy is considered when one node is active or operational and the other is not operating. During the breakdown of the active node, the passive node maintains availability by becoming the active node.




Replication
Replication = Redundancy + Synchronization

Active Replication
If a read-write operation is happening in a DB which is hosted on multiple nodes, read-write will happen in all the nodes and become synchronized.

Passive Replication
Every read-write -> master
If master goes down one slave becomes master.

Master-slave replication can be either synchronous or asynchronous. The difference is simply the timing of propagation of changes.
If the changes are made to the master and slave at the same time, it is synchronous.
If changes are queued up and written later, it is asynchronous.




Load Balancers
Load balancing is the process of efficient distribution of network traffic across all nodes in a distributed system.

Roles of load balancer
1. The load distribution is equal over every node.
2. Health check(if the node is not operational, the request is passed to another node that is up and running)
3. Load balancers ensure high scalability, high throughput and high availability.

When to use it and when not to use it?
In monolithic or in case of vertically scaled system, we don't need it. But in microservice architecture, we need it.

Challenges of load balancing
Single Point of Failure:
During a load balancer malfunctioning, the communication between clients and servers would be broken.
To solve this issue, we can use redundancy. The system can have an active load balancer and one passive load balancer.

Advantages of using load balancer
1. Optimization : Load balancers help in resource utlilization and lower response time, thereby optimizing the system in a high traffic environment
2. Better user experience : Load balancers help in reducing the latency and increasing availability making the user's request go smoothly and error-free.
3. Prevents Downtime : Load balancers maintain a record of servers that are non-operational and distribute the traffic accordingly, therefore ensuring security and preventing downtime, which also increases profit and productivity.
4. Flexibility : Load balancers have the flexibility to re-route the traffic in case of breakdown and work on server maintainance to ensure efficiency. This helps in avoiding traffic bottlenecks.
5. Scalability : When the traffic of a web application increases suddenly, load balancers can use physical and virtual servers to deliver the responses without any disruption.
6. Redundancy : Load balancing also provides in-build redundancy by re-routing the traffic in case of any failure.
